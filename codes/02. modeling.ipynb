{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836c4d1d",
   "metadata": {},
   "source": [
    "# 가상환경 프롬프트에서 set CUDA_VISIBLE_DEVICES=0 입력 후 코드 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "810b214d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll\n",
      "CUDA SETUP: CUDA runtime path found: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.7\\bin\\cudart64_110.dll\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\bitsandbytes\\libbitsandbytes_cuda117.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Users/khu/miniconda3/envs/manual/bin')}\n",
      "  warn(msg)\n",
      "C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: C:\\Users\\khu\\miniconda3\\envs\\manual did not contain ['cudart64_110.dll', 'cudart64_120.dll', 'cudart64_12.dll'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py:156: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {WindowsPath('C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.2/bin'), WindowsPath('C:/Users/khu/miniconda3/envs/manual/bin'), WindowsPath('C:/Users/khu/.dotnet/tools'), WindowsPath('C:/Users/khu/miniconda3/envs/manual/Library/usr/bin'), WindowsPath('C:/Users/khu/miniconda3/envs/manual/Library/mingw-w64/bin')}\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\transformers\\modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96dba83718c64eac9f69ab614f4efedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "C:\\Users\\khu\\miniconda3\\envs\\manual\\lib\\site-packages\\peft\\utils\\other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb66de0ee3d4371bd8dc12538dafba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2145' max='2145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2145/2145 20:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.890100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.921000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.616900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.448200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.113700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.880400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.170800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.887500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.866100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.774200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.033300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.864500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.896100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.763400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.775200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.887900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.785400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.914800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.917600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.886900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.008300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.991500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.727200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.927000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.874400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.844200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.676600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.826100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.958700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.828900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.849400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.889200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.737400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dbd277a61141cbba23114696ac64dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70126bd02d8a4aa9ac808bc71629efa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] What should I do if warning number 3003 is raised? [/INST] If warning number 3003 is raised, it means that the depth of cut is invalid. (Modify the machining program to specify an appropriate depth of cut.)\"\n",
      "This warning indicates that the value entered for the depth of cut is invalid. The depth of cut should be entered as a positive value. Modify the machining program to specify an appropriate depth of cut.\n",
      "Time taken: 51.0937 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset,load_dataset\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login, login\n",
    "import gc\n",
    "import transformers\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model = \"llama-2-7b-manual_automation_final_test\"\n",
    "path = 'C:\\\\Users\\\\khu\\\\Desktop\\\\manual\\\\automation\\\\'+new_model\n",
    "token = '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'\n",
    "lora_r = 64\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "num_train_epochs = 1\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "gradient_accumulation_steps = 1\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001\n",
    "optim = \"paged_adamw_32bit\"\n",
    "lr_scheduler_type = \"cosine\"\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "group_by_length = True\n",
    "save_steps = 0\n",
    "logging_steps = 25\n",
    "max_seq_length = 1024\n",
    "packing = False\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Functions\n",
    "def load_and_preprocess_data(file_path):\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df.dropna()\n",
    "\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    if \"__index_level_0__\" in dataset.column_names:\n",
    "        dataset = dataset.remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def fine_tune_llama_model(dataset ,output_dir= \"./results_final_test\", base_model_name=base_model_name):\n",
    "    compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=use_4bit,\n",
    "        bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    )\n",
    "\n",
    "    if compute_dtype == torch.float16 and use_4bit:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "        use_auth_token=token,\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True, use_auth_token=token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\" \n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=lora_alpha,\n",
    "        lora_dropout=lora_dropout,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        optim=optim,\n",
    "        save_steps=save_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        fp16=fp16,\n",
    "        bf16=bf16,\n",
    "        max_grad_norm=max_grad_norm,\n",
    "        max_steps=max_steps,\n",
    "        warmup_ratio=warmup_ratio,\n",
    "        group_by_length=group_by_length,\n",
    "        lr_scheduler_type=lr_scheduler_type,\n",
    "        report_to=\"tensorboard\"\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=packing,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.model.save_pretrained(new_model)\n",
    "\n",
    "    \n",
    "def run_tensorboard(log_dir=\"results/runs\"):\n",
    "    get_ipython().run_line_magic('load_ext', 'tensorboard')\n",
    "\n",
    "    get_ipython().run_line_magic('tensorboard', f'--logdir {log_dir}')\n",
    "\n",
    "\n",
    "def reload_and_merge_model(\n",
    "    trust_remote_code=True\n",
    "):\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        low_cpu_mem_usage=True,\n",
    "        return_dict=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "        use_auth_token=token,\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, path)\n",
    "    model = model.merge_and_unload()\n",
    "    model.save_pretrained(path)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=trust_remote_code, use_auth_token=token)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    tokenizer.save_pretrained(path)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def push_model_and_tokenizer_to_hub(model_path):\n",
    "    # token = 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\n",
    "    notebook_login()\n",
    "    \n",
    "    model.push_to_hub(model_path, use_temp_dir=False)\n",
    "    tokenizer.push_to_hub(model_path, use_temp_dir=False)\n",
    "\n",
    "    \n",
    "def empty_cache():\n",
    "    for obj in ['model', 'pipe', 'trainer']:\n",
    "        if obj in locals():\n",
    "            del locals()[obj]\n",
    "    gc.collect()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "def model_load():\n",
    "    model = AutoModelForCausalLM.from_pretrained(path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def tokenizer_load():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def model_use(model, tokenizer, prompt):    \n",
    "    pipeline = transformers.pipeline(\n",
    "        'text-generation',\n",
    "        model=model,\n",
    "        tokenizer = tokenizer,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map='auto',\n",
    "        max_length=200\n",
    "    )\n",
    "\n",
    "    start_time = time.time()  \n",
    "    result = pipeline(f\"[INST] {prompt} [/INST]\")\n",
    "    end_time = time.time() \n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(result[0]['generated_text'])\n",
    "    print(f\"Time taken: {elapsed_time:.4f} seconds\")\n",
    "    \n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = load_and_preprocess_data('C:\\\\Users\\\\khu\\\\Desktop\\\\manual\\\\data\\\\processed_ver3.xlsx')\n",
    "    fine_tune_llama_model(dataset , output_dir=\"./results_automation_test\")\n",
    "    # run_tensorboard(log_dir=\"results/runs\")\n",
    "    empty_cache()\n",
    "    reload_and_merge_model()\n",
    "    # push_model_and_tokenizer_to_hub(model_path=path)\n",
    "    model = model_load()\n",
    "    tokenizer = tokenizer_load()\n",
    "    model_use(model, tokenizer, 'What should I do if warning number 3003 is raised?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70293cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
